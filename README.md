# Project Overview

This repository contains a series of Jupyter notebooks developed for experimenting with different models related to visual question answering (VQA) systems. Each notebook is dedicated to a specific model or a variant of the Pix2Struct model.

## Notebooks Description

### `clip-blip-model.ipynb`

- **Purpose**: This notebook explores the integration of CLIP and BLIP models for enhanced image understanding and text generation. 
- **Features**: It includes model loading, data preprocessing, visualization of model outputs, and performance evaluation metrics.
- **Usage**: Ideal for understanding how combined models work in processing and answering visual questions.

### `pix2struct-Base.ipynb`

- **Purpose**: Focuses on the base version of the Pix2Struct model, examining its fundamental architectural components and performance.
- **Features**: Includes detailed steps for model setup, training procedures, evaluation, and visualization of results.
- **Usage**: Use this notebook to replicate the basic experiments or as a foundation for further experimentation with the Pix2Struct model.

### `pix2struct_7b.ipynb`

- **Purpose**: Dedicated to a smaller variant of the Pix2Struct model, known as Pix2Struct-7B, which is optimized for faster computations.
- **Features**: Outlines the adjustments made to the base model to enhance speed and efficiency, and compares performance metrics with the base model.
- **Usage**: Best for scenarios where computational resources are limited or where quick prototyping is required.

## Getting Started

To run these notebooks:
1. Ensure that Python 3.x is installed on your machine.
2. Install Jupyter Notebook or JupyterLab:
